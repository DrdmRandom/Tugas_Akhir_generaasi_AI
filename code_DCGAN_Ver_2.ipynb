{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "###  Importing Library",
   "id": "3ede05b1f5de5c6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-26T13:09:12.056108Z",
     "start_time": "2024-12-26T13:09:12.038628Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import make_grid, save_image\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "import numpy as np\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n"
   ],
   "id": "284c0317455743cf",
   "outputs": [],
   "execution_count": 36
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 1. Hyperparameters & Device",
   "id": "f61e59719459ca80"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-26T13:09:12.071331Z",
     "start_time": "2024-12-26T13:09:12.064953Z"
    }
   },
   "cell_type": "code",
   "source": [
    "batch_size = 128\n",
    "image_size = 128       # resolusi gambar (64x64)\n",
    "nz = 128              # dimensi vektor noise (input generator)\n",
    "num_epochs = 300      # ganti sesuai kebutuhan\n",
    "lr = 0.0005\n",
    "beta1 = 0.5\n",
    "ngf = 64  # generator feature map size\n",
    "ndf = 64  # discriminator feature map size\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU Name:\", torch.cuda.get_device_name(0))\n",
    "    print(\"CUDA Version:\", torch.version.cuda)"
   ],
   "id": "d30a40acd9f72211",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available: True\n",
      "GPU Name: NVIDIA GeForce GTX 1060 6GB\n",
      "CUDA Version: 11.8\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 2. Dataset & DataLoader (dengan Augmentations)",
   "id": "3877425e00b1a2d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-26T13:09:12.741974Z",
     "start_time": "2024-12-26T13:09:12.727860Z"
    }
   },
   "cell_type": "code",
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize(image_size),\n",
    "    transforms.CenterCrop(image_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5]*3, [0.5]*3)\n",
    "])"
   ],
   "id": "a5a69315ab456136",
   "outputs": [],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-26T13:09:13.944115Z",
     "start_time": "2024-12-26T13:09:12.752685Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data_dir = r\"C:\\Users\\dawwi\\Downloads\\Dataset_3\"  # sesuaikan path\n",
    "dataset = torchvision.datasets.ImageFolder(root=data_dir, transform=transform)\n",
    "\n",
    "indices = list(range(len(dataset)))\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "# Misal pakai subset 1000 data\n",
    "subset_indices = indices[:1000]\n",
    "sampler = SubsetRandomSampler(subset_indices)\n",
    "\n",
    "# DataLoader (menggunakan sampler)\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=batch_size,\n",
    "    sampler=sampler,\n",
    "    num_workers=4\n",
    ")"
   ],
   "id": "6e95c2e324e10f98",
   "outputs": [],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-26T13:09:14.412379Z",
     "start_time": "2024-12-26T13:09:14.399055Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# data_dir = r\"C:\\Users\\dawwi\\Downloads\\Dataset\"  # ganti dengan path dataset Anda\n",
    "#\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.Resize(image_size),\n",
    "#     transforms.CenterCrop(image_size),\n",
    "#     transforms.RandomRotation(degrees=15),\n",
    "#     transforms.RandomHorizontalFlip(p=0.5),\n",
    "#     transforms.ColorJitter(\n",
    "#         brightness=(0.2, 0.6),   # contoh range brightness\n",
    "#         contrast=(0.2, 0.8),\n",
    "#         saturation=(0.2, 0.7),\n",
    "#         hue=(-0.1, 0.1)\n",
    "#     ),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize([0.5]*3, [0.5]*3)\n",
    "# ])\n",
    "#\n",
    "#\n",
    "# dataset = torchvision.datasets.ImageFolder(root=data_dir, transform=transform)\n",
    "# dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=2)"
   ],
   "id": "72f8bf976350287d",
   "outputs": [],
   "execution_count": 40
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 3. Definisi Model: Generator & Discriminator",
   "id": "6e529928a25dec17"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-26T13:09:15.178289Z",
     "start_time": "2024-12-26T13:09:15.163896Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, nz, ngf, nc=3):\n",
    "        super(Generator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.ConvTranspose2d(nz, ngf*8, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(ngf*8),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.ConvTranspose2d(ngf*8, ngf*4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf*4),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.ConvTranspose2d(ngf*4, ngf*2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf*2),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.ConvTranspose2d(ngf*2, ngf, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.ConvTranspose2d(ngf, nc, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.main(x)"
   ],
   "id": "6cc743361af95e44",
   "outputs": [],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-26T13:09:15.240917Z",
     "start_time": "2024-12-26T13:09:15.183907Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, ndf, nc=3):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(ndf, ndf*2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf*2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(ndf*2, ndf*4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf*4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(ndf*4, ndf*8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf*8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(ndf*8, 1, 4, 1, 0, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.main(x)\n",
    "\n",
    "netG = Generator(nz, ngf).to(device)\n",
    "netD = Discriminator(ndf).to(device)"
   ],
   "id": "873573f297c229c1",
   "outputs": [],
   "execution_count": 42
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 4. Loss, Optimizer",
   "id": "fbd0cfb82344fd64"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-26T13:09:15.598188Z",
     "start_time": "2024-12-26T13:09:15.584388Z"
    }
   },
   "cell_type": "code",
   "source": [
    "criterion = nn.BCELoss()\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "\n",
    "lossD_history = []\n",
    "lossG_history = []\n",
    "\n",
    "# noise tetap (untuk sample gambar di tiap epoch)\n",
    "fixed_noise = torch.randn(batch_size, nz, 1, 1, device=device)\n",
    "\n",
    "print(\"Start Training...\")\n",
    "num_iter = 0\n",
    "\n",
    "real_label = 1.0\n",
    "fake_label = 0.0\n",
    "\n",
    "os.makedirs(\"samples\", exist_ok=True)"
   ],
   "id": "5534f53ee0dfbdd8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training...\n"
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 5. Training Loop",
   "id": "7f2791bfc6ecb0c0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-26T13:09:33.700501Z",
     "start_time": "2024-12-26T13:09:15.942723Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for i, (imgs, _) in enumerate(dataloader):\n",
    "        # (A) Train Discriminator\n",
    "        netD.zero_grad()\n",
    "\n",
    "        real_imgs = imgs.to(device)\n",
    "        b_size = real_imgs.size(0)\n",
    "\n",
    "        label_real = torch.full((b_size,), real_label, dtype=torch.float, device=device)\n",
    "        label_fake = torch.full((b_size,), fake_label, dtype=torch.float, device=device)\n",
    "\n",
    "        output_real = netD(real_imgs).view(-1)\n",
    "        lossD_real = criterion(output_real, label_real)\n",
    "\n",
    "        noise = torch.randn(b_size, nz, 1, 1, device=device)\n",
    "        fake_imgs = netG(noise)\n",
    "        output_fake = netD(fake_imgs.detach()).view(-1)\n",
    "        lossD_fake = criterion(output_fake, label_fake)\n",
    "\n",
    "        lossD_total = lossD_real + lossD_fake\n",
    "        lossD_total.backward()\n",
    "        optimizerD.step()\n",
    "\n",
    "        # (B) Train Generator\n",
    "        netG.zero_grad()\n",
    "        # Generator wants D(G(z)) == 1\n",
    "        output_fake_for_G = netD(fake_imgs).view(-1)\n",
    "        lossG_total = criterion(output_fake_for_G, label_real)\n",
    "        lossG_total.backward()\n",
    "        optimizerG.step()\n",
    "\n",
    "        lossD_history.append(lossD_total.item())\n",
    "        lossG_history.append(lossG_total.item())\n",
    "\n",
    "        num_iter += 1\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}] | Batch [{i}/{len(dataloader)}] | \"\n",
    "                  f\"LossD: {lossD_total.item():.4f} | LossG: {lossG_total.item():.4f}\")\n",
    "\n",
    "    # Simpan sample di akhir epoch\n",
    "    with torch.no_grad():\n",
    "        fake = netG(fixed_noise).detach().cpu()\n",
    "    grid = make_grid(fake, nrow=8, normalize=True)\n",
    "    save_image(grid, f\"samples/epoch_{epoch+1}.png\")\n",
    "    print(f\"Sample image saved: samples/epoch_{epoch+1}.png\")\n",
    "\n",
    "print(\"Training Finished.\")"
   ],
   "id": "6bfc4cb60daebfd8",
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Using a target size (torch.Size([128])) that is different to the input size (torch.Size([3200])) is deprecated. Please ensure they have the same size.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[44], line 13\u001B[0m\n\u001B[0;32m     10\u001B[0m label_fake \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mfull((b_size,), fake_label, dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mfloat, device\u001B[38;5;241m=\u001B[39mdevice)\n\u001B[0;32m     12\u001B[0m output_real \u001B[38;5;241m=\u001B[39m netD(real_imgs)\u001B[38;5;241m.\u001B[39mview(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m---> 13\u001B[0m lossD_real \u001B[38;5;241m=\u001B[39m \u001B[43mcriterion\u001B[49m\u001B[43m(\u001B[49m\u001B[43moutput_real\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabel_real\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     15\u001B[0m noise \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mrandn(b_size, nz, \u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m1\u001B[39m, device\u001B[38;5;241m=\u001B[39mdevice)\n\u001B[0;32m     16\u001B[0m fake_imgs \u001B[38;5;241m=\u001B[39m netG(noise)\n",
      "File \u001B[1;32m~\\OneDrive\\Desktop\\tubes_Generativ_AI\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\OneDrive\\Desktop\\tubes_Generativ_AI\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32m~\\OneDrive\\Desktop\\tubes_Generativ_AI\\.venv\\lib\\site-packages\\torch\\nn\\modules\\loss.py:697\u001B[0m, in \u001B[0;36mBCELoss.forward\u001B[1;34m(self, input, target)\u001B[0m\n\u001B[0;32m    696\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor, target: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m--> 697\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbinary_cross_entropy\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    698\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarget\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreduction\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreduction\u001B[49m\n\u001B[0;32m    699\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\OneDrive\\Desktop\\tubes_Generativ_AI\\.venv\\lib\\site-packages\\torch\\nn\\functional.py:3545\u001B[0m, in \u001B[0;36mbinary_cross_entropy\u001B[1;34m(input, target, weight, size_average, reduce, reduction)\u001B[0m\n\u001B[0;32m   3543\u001B[0m     reduction_enum \u001B[38;5;241m=\u001B[39m _Reduction\u001B[38;5;241m.\u001B[39mget_enum(reduction)\n\u001B[0;32m   3544\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m target\u001B[38;5;241m.\u001B[39msize() \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28minput\u001B[39m\u001B[38;5;241m.\u001B[39msize():\n\u001B[1;32m-> 3545\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m   3546\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUsing a target size (\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtarget\u001B[38;5;241m.\u001B[39msize()\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m) that is different to the input size (\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28minput\u001B[39m\u001B[38;5;241m.\u001B[39msize()\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m) is deprecated. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   3547\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPlease ensure they have the same size.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   3548\u001B[0m     )\n\u001B[0;32m   3550\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m weight \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m   3551\u001B[0m     new_size \u001B[38;5;241m=\u001B[39m _infer_size(target\u001B[38;5;241m.\u001B[39msize(), weight\u001B[38;5;241m.\u001B[39msize())\n",
      "\u001B[1;31mValueError\u001B[0m: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([3200])) is deprecated. Please ensure they have the same size."
     ]
    }
   ],
   "execution_count": 44
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 6. Plot Line Chart (Loss vs. Iteration)",
   "id": "d7013414a105c6f7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"Training Loss (Discriminator & Generator)\")\n",
    "plt.plot(lossD_history, label=\"LossD\")\n",
    "plt.plot(lossG_history, label=\"LossG\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.savefig(\"training_loss_plot.png\")\n",
    "plt.show()\n",
    "print(\"Loss plot saved: training_loss_plot.png\")"
   ],
   "id": "9218e374f08dfbe4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 7. Visualisasi t-SNE di Latent Noise",
   "id": "ad78402b25d0b7e9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def visualize_latent_tsne(modelG, num_samples=500):\n",
    "    modelG.eval()\n",
    "    all_z = []\n",
    "    for _ in range(num_samples):\n",
    "        z = torch.randn(1, nz, 1, 1, device=device)\n",
    "        # flatten\n",
    "        all_z.append(z.view(1, -1).cpu().numpy())\n",
    "    all_z = np.concatenate(all_z, axis=0)  # shape (num_samples, nz)\n",
    "\n",
    "    print(\"Running t-SNE on latent vectors...\")\n",
    "    tsne = TSNE(n_components=2, perplexity=30, n_iter=1000)\n",
    "    z_2d = tsne.fit_transform(all_z)\n",
    "\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.scatter(z_2d[:,0], z_2d[:,1], alpha=0.7, s=10, c='blue')\n",
    "    plt.title(\"t-SNE of Random Noise Z (DCGAN)\")\n",
    "    plt.xlabel(\"Dimension 1\")\n",
    "    plt.ylabel(\"Dimension 2\")\n",
    "    plt.show()\n",
    "    print(\"t-SNE plot displayed.\")"
   ],
   "id": "8c769a50ca5341d7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 8. Eval Function (Contoh)",
   "id": "2c20ab08e78b3678"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def evaluate_model(generator, device, num_images=16, save_path=\"eval_samples.png\"):\n",
    "    \"\"\"\n",
    "    Contoh evaluasi sederhana: generate sejumlah gambar fake,\n",
    "    lalu simpan dalam grid.\n",
    "    \"\"\"\n",
    "    generator.eval()\n",
    "    with torch.no_grad():\n",
    "        noise = torch.randn(num_images, nz, 1, 1, device=device)\n",
    "        fake_images = generator(noise).cpu()\n",
    "    grid = make_grid(fake_images, nrow=4, normalize=True)\n",
    "    save_image(grid, save_path)\n",
    "    print(f\"Evaluation samples saved: {save_path}\")"
   ],
   "id": "5c7fd3ef7a867e0b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
